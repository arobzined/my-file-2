{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the dataset is too big , i only put this code here to show how i implement it , i saved the data as data.csv .\n",
    "\"\"\"\n",
    "neg_data_list = []\n",
    "pos_data_list = []\n",
    "\n",
    "neg_dir = 'data\\\\train\\\\neg'\n",
    "pos_dir = 'data\\\\train\\\\pos'\n",
    "\n",
    "for filename in os.listdir(neg_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(neg_dir, filename)\n",
    "        with open(file_path, 'r',encoding=\"utf-8\") as file:\n",
    "            file_content = file.read()\n",
    "            neg_data_list.append(file_content)\n",
    "\n",
    "\n",
    "for filename in os.listdir(pos_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(pos_dir, filename)\n",
    "        with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "            file_content = file.read()\n",
    "            pos_data_list.append(file_content)\n",
    "\n",
    "n_df = pd.DataFrame(neg_data_list, columns=['Data'])\n",
    "p_df = pd.DataFrame(pos_data_list, columns=['Data'])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.info of                                                     Data  Value\n",
      "0      Silent Night, Deadly Night 5 is the very last ...      0\n",
      "1      The idea ia a very short film with a lot of in...      1\n",
      "2      For me, this movie just seemed to fall on its ...      0\n",
      "3      Was this based on a comic-book? A video-game? ...      1\n",
      "4      Caution: May contain spoilers...<br /><br />I'...      1\n",
      "...                                                  ...    ...\n",
      "24995  \"Winchester '73\" marked the first of a series ...      1\n",
      "24996  Was it foreshadowing when Tori complained that...      0\n",
      "24997  This movie is horrible! It rivals \"Ishtar\" in ...      0\n",
      "24998  Abhay Deol meets the attractive Soha Ali Khan ...      1\n",
      "24999  Hilarious film. I saw this film at the 2002 Sy...      1\n",
      "\n",
      "[25000 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "n_df[\"Value\"] = 0\n",
    "p_df[\"Value\"] = 1\n",
    "\n",
    "base_df = pd.concat([n_df,p_df])\n",
    "df = base_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df.head(10)\n",
    "\n",
    "# df = df.head(4000)\n",
    "\n",
    "df.to_csv('data.csv')\n",
    "\"\"\"\n",
    "# What we have done:\n",
    "# * we read the dataset (both negative and positive values) , and store them in the lists .\n",
    "# * then we create two dataframes and put them wherever they are belong to . \n",
    "# * we \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # tokenazing the text into words\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # remove stopwords (unnecessary words)\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    \n",
    "    # reconstruct the preprocessed text and return it\n",
    "    preprocessed_text = \" \".join(words)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\deniz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\deniz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "df = df.head(4000)\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "df['Data'] = df['Data'].apply(preprocess_text)\n",
    "\n",
    "# Defining X and y variables --> X : Datas , y : True Sentiment results of itself\n",
    "X = df['Data']\n",
    "y = df['Value']\n",
    "\n",
    "# convert the features (datas) into a numerical format using TF-IDF vectorization\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "# split the data into training and testing sets (just for the train part , we are going to test it with test dataset too.)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train a classifier (Linear Support Vector Classifier in this case)\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\") # accuracy of the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[350  72]\n",
      " [ 37 341]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.87       422\n",
      "           1       0.83      0.90      0.86       378\n",
      "\n",
      "    accuracy                           0.86       800\n",
      "   macro avg       0.87      0.87      0.86       800\n",
      "weighted avg       0.87      0.86      0.86       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# calculate the confusion matrix to compare the results\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# calculate the classification -> precision, recall, and F1-score\n",
    "class_report = classification_report(y_test, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:\n",
      "[0.85875 0.85125 0.85625 0.86125 0.85375]\n",
      "Mean Cross-validation Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# performing 5-fold cross-validation for chechking the data if it is evenly distributed.\n",
    "cross_val_scores = cross_val_score(classifier, X, y, cv=5)\n",
    "print(\"Cross-validation scores:\")\n",
    "print(cross_val_scores)\n",
    "print(f\"Mean Cross-validation Accuracy: {cross_val_scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on the new test dataset: 0.85\n"
     ]
    }
   ],
   "source": [
    "# Since the dataset is too large , i only wanted to show the code but i saved the dataset as data_test.csv :D\n",
    "\"\"\"\n",
    "neg_data_list = []\n",
    "pos_data_list = []\n",
    "\n",
    "neg_test_dir = 'data\\\\test\\\\neg'\n",
    "pos_test_dir = 'data\\\\test\\\\pos'\n",
    "\n",
    "for filename in os.listdir(neg_test_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(neg_test_dir, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            file_content = file.read()\n",
    "            neg_data_list.append(file_content)\n",
    "\n",
    "\n",
    "for filename in os.listdir(pos_test_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(pos_test_dir, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            file_content = file.read()\n",
    "            pos_data_list.append(file_content)\n",
    "\n",
    "n_tdf = pd.DataFrame(neg_data_list, columns=['Data'])\n",
    "p_tdf = pd.DataFrame(pos_data_list, columns=['Data'])\n",
    "\n",
    "n_tdf[\"Value\"] = 0\n",
    "p_tdf[\"Value\"] = 1\n",
    "\n",
    "base_tdf = pd.concat([n_tdf,p_tdf])\n",
    "df_t = base_tdf.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df_t = pd.read_csv(\"data_test.csv\")\n",
    "df_t['Data'] = df_t['Data'].apply(preprocess_text)\n",
    "\n",
    "X_new = df_t['Data']\n",
    "y_new = df_t['Value']\n",
    "\n",
    "X_new = tfidf_vectorizer.transform(X_new)\n",
    "\n",
    "predictions_new = classifier.predict(X_new)\n",
    "\n",
    "accuracy_new = accuracy_score(y_new, predictions_new)\n",
    "print(f\"accuracy on the new test dataset: {accuracy_new:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[10536  1964]\n",
      " [ 1773 10727]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.87       422\n",
      "           1       0.83      0.90      0.86       378\n",
      "\n",
      "    accuracy                           0.86       800\n",
      "   macro avg       0.87      0.87      0.86       800\n",
      "weighted avg       0.87      0.86      0.86       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "conf_matrix = confusion_matrix(y_new, predictions_new)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "class_report = classification_report(y_test, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:\n",
      "[0.901  0.8948 0.8936 0.8984 0.8884]\n",
      "Mean Cross-validation Accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_scores = cross_val_score(classifier, X_new, y_new, cv=5)\n",
    "print(\"Cross-validation scores:\")\n",
    "print(cross_val_scores)\n",
    "print(f\"Mean Cross-validation Accuracy: {cross_val_scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hated movie !\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "with open('model.pkl', 'rb') as f:\n",
    "    new_classifier = pickle.load(f)\n",
    "\n",
    "exm = preprocess_text(\"i hated that movie!\")\n",
    "\n",
    "print(exm)\n",
    "example_input_vector = tfidf_vectorizer.transform([exm])\n",
    "prediction = new_classifier.predict(example_input_vector)\n",
    "print(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "def result(x):\n",
    "    if x == 1:\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Negative\"\n",
    "\n",
    "print(result(prediction[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
